
Tips for linear regression------

Could you imagine if you had a dataset that has a few outliers?
Least Squares Regression is susceptible to outliers; one could imagine that that would impact the calculation of the average, if you have some extreme outliers. Suppose your dataset isn't that big either, then it is even more susceptible to outliers, making the linear regression algorithm perform poorly.
When performing linear regression, we have to be wary of overfitting. It is too easy to use too complicated of a model, by the formula.
It might be a clear indication of overfitting, if you use too many input variables in linear regression, such that less input variables will give a better model.
Finally, always remember to evaluate your model using new data, that you did not train you model on. If you evaluate on the same data as you trained your model, you could get a different picture of which model is the best for new data.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Example: Correlation and Causation
___________________________________
Just because there’s a strong correlation between two variables, there isn’t necessarily a causal relationship between them. 
For example, drowning deaths and ice-cream sales are strongly correlated, but that’s because both are affected by the season (summer vs. winter). 
In general, there are several possible cases, as illustrated below:
 a) Causal link: Even
if there is a causal link between x and y, correlation alone cannot tell us whether y causes x or

(b) Hidden Cause:
A hidden variable z causes both x and y, creating the correlation.

(c) Confounding
Factor: A hidden variable z and x both affect y, so the results also depend on the value of z.
x y
(d) Coincidence:
The correlation just happened by chance (e.g. the strong correlation between sun cycles and number of Republicans in Congress)

Important Link : 
http://www.mit.edu/~6.s085/notes/lecture3.pdf

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Interpolation vs. extrapolation
________________________________
As a reminder, everything here crucially depends on the probabilistic model being true. 
In practice, when we do prediction for some value of x we haven’t seen before, we need to be very careful. Predicting y for a value of x that is within the interval of points that we saw in the original data (the data that we fit our model with) is called
interpolation. Predicting y for a value of x that’s outside the range of values we actually saw for x in the original data is called extrapolation. For real datasets, even if a linear fit seems appropriate, we need to be extremely careful
about extrapolation, which can often lead to false predictions!

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Correlation Coefficient, r :
_____________________________
   The quantity r, called the linear correlation coefficient, measures the strength and
   the direction of a linear relationship between two variables. The linear correlation
   coefficient is sometimes referred to as the Pearson product moment correlation coefficient in honor of its developer Karl Pearson.
   
   The value of r is such that -1 < r < +1.  The + and – signs are used for positive
      linear correlations and negative linear correlations, respectively. 
   Positive correlation:    If x and y have a strong positive linear correlation, r is close
      to +1.  An r value of exactly +1 indicates a perfect positive fit.   Positive values
      indicate a relationship between x and y variables such that as values for x increases,
      values for  y also increase.
   Negative correlation:   If x and y have a strong negative linear correlation, r is close
     to -1.  An r value of exactly -1 indicates a perfect negative fit.   Negative values
     indicate a relationship between x and y such that as values for x increase, values
     for y decrease.
   No correlation:  If there is no linear correlation or a weak linear correlation, r is
     close to 0.  A value near zero means that there is a random, nonlinear relationship
     between the two variables
   Note that r is a dimensionless quantity; that is, it does not depend on the units
     employed.
   A perfect correlation of ± 1 occurs only when the data points all lie exactly on a
     straight line.  If r = +1, the slope of this line is positive.  If r = -1, the slope of this
     line is negative. 
   A correlation greater than 0.8 is generally described as strong, whereas a correlation
      less than 0.5 is generally described as weak.  These values can vary based upon the
     "type" of data being examined.  A study utilizing scientific data may require a stronger
      correlation than a study using social science data.  
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Coefficient of Determination, r 2  or  R2 :
_______________________________________
   The coefficient of determination, r 2, is useful because it gives the proportion of the variance (fluctuation) of one variable that    is predictable from the other variable.It is a measure that allows us to determine how certain one can be in making predictions from a   certain model/graph.The coefficient of determination is the ratio of the explained variation to the total variation.
   The coefficient of determination is such that 0 <  r 2 < 1,  and denotes the strengthof the linear association between x and y. 
   The coefficient of determination represents the percent of the data that is the closest
      to the line of best fit.  For example, if r = 0.922, then r 2 = 0.850, which means that
      85% of the total variation in y can be explained by the linear relationship between x
      and y (as described by the regression equation).  The other 15% of the total variation
      in y remains unexplained.
   The coefficient of determination is a measure of how well the regression line
      represents the data.  If the regression line passes exactly through every point on the
      scatter plot, it would be able to explain all of the variation. The further the line is
      away from the points, the less it is able to explain.
